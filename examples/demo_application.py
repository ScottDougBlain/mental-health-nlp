"""
Mental Health NLP Demo Application

A comprehensive demonstration of responsible AI development in sensitive mental health domains.
This application showcases the LSTM suicide risk detection model with extensive safety guardrails.

IMPORTANT: For educational and research purposes only. Not for clinical use.
"""

import os
import sys
import argparse
import warnings
from pathlib import Path

# Add src to path for imports
sys.path.append(str(Path(__file__).parent.parent / "src"))

from suicide_risk_detector import SuicideRiskDetector, EthicalWarning
from safety_guidelines import SafetyGuidelines, SafetyMonitor
import torch
import numpy as np
import matplotlib.pyplot as plt


class MentalHealthNLPDemo:
    """
    Comprehensive demo application for mental health NLP with safety features.

    This demo showcases:
    - Responsible AI development practices
    - Comprehensive safety protocols
    - Ethical guidelines implementation
    - Crisis intervention procedures
    - Performance monitoring and audit trails
    """

    def __init__(self):
        """Initialize the demo application."""
        print("üß† Mental Health NLP Safety Demo")
        print("=" * 50)

        # Initialize components
        self.detector = SuicideRiskDetector()
        self.safety_guidelines = SafetyGuidelines()
        self.safety_monitor = SafetyMonitor()

        # Demo settings
        self.demo_mode = True
        self.show_visualizations = True

        # Display initial warnings
        self._display_demo_introduction()

    def _display_demo_introduction(self):
        """Display comprehensive demo introduction."""
        intro_text = """
        üéì EDUCATIONAL DEMONSTRATION

        This demo showcases responsible AI development for mental health applications.

        DEMO FEATURES:
        ‚Ä¢ LSTM model architecture for suicide risk detection (demo mode)
        ‚Ä¢ Comprehensive safety protocols and ethical guidelines
        ‚Ä¢ Crisis intervention procedures and resource provision
        ‚Ä¢ Performance monitoring and audit capabilities
        ‚Ä¢ Responsible AI practices for sensitive domains

        NOTE: This is a demonstration system showing architecture and safety patterns.
        Actual model performance requires training with appropriate clinical data.

        LEARNING OBJECTIVES:
        ‚Ä¢ Understanding ethical AI development in healthcare
        ‚Ä¢ Implementing safety guardrails for high-stakes applications
        ‚Ä¢ Balancing model performance with responsible deployment
        ‚Ä¢ Demonstrating technical skills with social responsibility

        SAFETY NOTICE:
        This is a demonstration system only. Real mental health applications require:
        ‚Ä¢ Clinical validation and regulatory approval
        ‚Ä¢ Professional oversight and intervention protocols
        ‚Ä¢ Comprehensive privacy and security measures
        ‚Ä¢ Ongoing monitoring and bias assessment

        """
        print(intro_text)

    def run_demo_mode(self):
        """Run comprehensive demo showcasing all features."""
        print("\nüöÄ STARTING COMPREHENSIVE DEMO")
        print("=" * 50)

        # 1. Safety Guidelines Demo
        self._demo_safety_guidelines()

        # 2. Model Architecture Demo
        self._demo_model_architecture()

        # 3. Prediction Demo with Safety Protocols
        self._demo_predictions_with_safety()

        # 4. Performance Analysis Demo
        self._demo_performance_analysis()

        # 5. Ethical Considerations Demo
        self._demo_ethical_considerations()

        # 6. Crisis Resources Demo
        self._demo_crisis_resources()

        print("\n‚úÖ DEMO COMPLETED SUCCESSFULLY")
        print("This demonstration showcases responsible AI development practices")
        print("for sensitive mental health applications.")

    def _demo_safety_guidelines(self):
        """Demonstrate safety guidelines and protocols."""
        print("\nüìã SAFETY GUIDELINES DEMONSTRATION")
        print("-" * 30)

        # Validate use case
        validation = self.safety_guidelines.validate_use_case("Educational demonstration")
        print(f"Use case validation: {'‚úÖ APPROVED' if validation['approved'] else '‚ùå REJECTED'}")

        if validation['requirements']:
            print("Requirements for this use case:")
            for req in validation['requirements']:
                print(f"  ‚Ä¢ {req}")

        # Show ethics checklist
        print("\nEthics checklist categories:")
        ethics_checklist = self.safety_guidelines.generate_ethics_checklist()
        for category, items in ethics_checklist.items():
            print(f"  üìã {category.replace('_', ' ').title()}: {len(items)} items")

    def _demo_model_architecture(self):
        """Demonstrate model architecture and capabilities."""
        print("\nüèóÔ∏è MODEL ARCHITECTURE DEMONSTRATION")
        print("-" * 30)

        # Create demo model (untrained) for demonstration
        self.detector.create_demo_model()

        print("\n‚ö†Ô∏è NOTE: Using untrained demo model for architecture illustration")
        print("Full functionality requires training with appropriate data")

        # Show model details
        total_params = sum(p.numel() for p in self.detector.model.parameters())
        trainable_params = sum(p.numel() for p in self.detector.model.parameters() if p.requires_grad)

        print(f"Model: Bidirectional LSTM")
        print(f"Total parameters: {total_params:,}")
        print(f"Trainable parameters: {trainable_params:,}")
        print(f"Vocabulary size: {self.detector.preprocessor.vocab_size:,}")
        print(f"Max sequence length: {self.detector.max_length}")

        # Show model summary
        print(f"\nModel Architecture:")
        print(f"  ‚Ä¢ Embedding Layer: {self.detector.preprocessor.vocab_size} ‚Üí 128")
        print(f"  ‚Ä¢ Bidirectional LSTM: 128 ‚Üí 64 (√ó2 layers)")
        print(f"  ‚Ä¢ Dropout: 30%")
        print(f"  ‚Ä¢ Dense Layer: 128 ‚Üí 2")
        print(f"  ‚Ä¢ Output: Suicide Risk Probability")

    def _demo_predictions_with_safety(self):
        """Demonstrate predictions with comprehensive safety protocols."""
        print("\nüîç PREDICTION WITH SAFETY PROTOCOLS DEMONSTRATION")
        print("-" * 30)

        # Demo test cases with varying risk levels
        demo_cases = [
            {
                "text": "Feeling really great today! Just got a promotion and excited about the future.",
                "expected_risk": "low",
                "description": "Positive, optimistic text"
            },
            {
                "text": "Having a tough day but I know things will get better with time and support.",
                "expected_risk": "low",
                "description": "Mild negative but resilient text"
            },
            {
                "text": "Everything feels overwhelming lately and I'm not sure how to cope anymore.",
                "expected_risk": "moderate",
                "description": "Distressed but not explicitly harmful"
            },
            {
                "text": "I don't want to be here anymore, everything hurts too much to continue.",
                "expected_risk": "high",
                "description": "High-risk suicidal ideation (DEMO ONLY)"
            }
        ]

        for i, case in enumerate(demo_cases, 1):
            print(f"\n--- Demo Case {i}: {case['description']} ---")
            print(f"Text: \"{case['text']}\"")

            # Simulate prediction (since we don't have a trained model)
            prediction, confidence = self._simulate_prediction(case['expected_risk'])

            print(f"Prediction: {'Risk Detected' if prediction == 1 else 'No Risk'}")
            print(f"Confidence: {confidence:.1%}")

            # Execute safety protocol
            risk_level = self.safety_guidelines.assess_risk_level(prediction, confidence)
            safety_results = self.safety_guidelines.execute_safety_protocol(
                risk_level, case['text'], {
                    'prediction': prediction,
                    'confidence': confidence,
                    'timestamp': 'demo_time'
                }
            )

            print(f"Risk Level: {risk_level.value.upper()}")
            print(f"Protocol Executed: {'‚úÖ' if safety_results['protocol_executed'] else '‚ùå'}")

            if safety_results['warnings_issued']:
                print("‚ö†Ô∏è Warnings:")
                for warning in safety_results['warnings_issued']:
                    print(f"  ‚Ä¢ {warning}")

            if safety_results['resources_provided']:
                print("üÜò Resources provided:")
                for resource in safety_results['resources_provided'][:2]:  # Show first 2
                    print(f"  ‚Ä¢ {resource}")

            # Log for monitoring
            self.safety_monitor.log_prediction(
                {'confidence': confidence, 'timestamp': 'demo_time'},
                safety_results
            )

    def _simulate_prediction(self, expected_risk: str) -> tuple:
        """Simulate model prediction based on expected risk level."""
        # This simulates different confidence levels for demo purposes
        if expected_risk == "low":
            return 0, np.random.uniform(0.7, 0.95)  # High confidence, no risk
        elif expected_risk == "moderate":
            return 1, np.random.uniform(0.5, 0.65)  # Moderate confidence, risk
        elif expected_risk == "high":
            return 1, np.random.uniform(0.75, 0.9)  # High confidence, risk
        else:
            return 0, 0.5

    def _demo_performance_analysis(self):
        """Demonstrate performance analysis and monitoring."""
        print("\nüìä PERFORMANCE ANALYSIS DEMONSTRATION")
        print("-" * 30)

        # Show safety metrics from demo predictions
        safety_metrics = self.safety_monitor.get_safety_metrics()

        if 'error' not in safety_metrics:
            print("Safety Monitoring Results:")
            print(f"  ‚Ä¢ Total incidents processed: {safety_metrics['total_incidents']}")
            print(f"  ‚Ä¢ High-risk incident rate: {safety_metrics['high_risk_rate']:.1%}")
            print(f"  ‚Ä¢ Human oversight rate: {safety_metrics['human_oversight_rate']:.1%}")
            print(f"  ‚Ä¢ Protocol compliance: {safety_metrics['safety_protocol_compliance']:.1%}")

            print(f"\nRisk Level Distribution:")
            for level, count in safety_metrics['risk_distribution'].items():
                percentage = (count / safety_metrics['total_incidents']) * 100
                print(f"  ‚Ä¢ {level.title()}: {count} ({percentage:.1f}%)")

        # Show target model performance metrics
        print(f"\nTarget Model Performance Metrics (literature benchmarks):")
        print(f"  ‚Ä¢ Target Accuracy: >90%")
        print(f"  ‚Ä¢ Target F1 Score: >0.90")
        print(f"  ‚Ä¢ Target Precision: >85%")
        print(f"  ‚Ä¢ Target Recall: >90%")
        print(f"  ‚Ä¢ Target AUC-ROC: >0.95")
        print(f"\nNote: These are target benchmarks based on published research.")
        print(f"Actual performance depends on training data quality and volume.")

    def _demo_ethical_considerations(self):
        """Demonstrate ethical considerations and responsible AI practices."""
        print("\n‚öñÔ∏è ETHICAL CONSIDERATIONS DEMONSTRATION")
        print("-" * 30)

        print("Key Ethical Principles Implemented:")

        ethical_principles = [
            "üîí Privacy Protection: Text hashing for logs, no raw text storage",
            "üéØ Bias Mitigation: Diverse training data and fairness assessment",
            "üë• Human Oversight: Required for all high-risk predictions",
            "üìñ Transparency: Clear model limitations and uncertainty quantification",
            "üö® Crisis Intervention: Immediate escalation protocols for emergencies",
            "üîç Continuous Monitoring: Ongoing performance and bias assessment",
            "üìö Professional Standards: Adherence to clinical ethics guidelines",
            "ü§ù Stakeholder Engagement: Input from mental health professionals"
        ]

        for principle in ethical_principles:
            print(f"  ‚Ä¢ {principle}")

        print(f"\nRisk Mitigation Strategies:")
        risk_mitigations = [
            "False Positives: Provide resources without alarm, human verification",
            "False Negatives: Comprehensive screening, multiple assessment points",
            "Bias: Regular fairness audits across demographic groups",
            "Privacy: Minimal data collection, encryption, access controls",
            "Misuse: Clear usage guidelines, prohibited use enforcement"
        ]

        for mitigation in risk_mitigations:
            print(f"  ‚Ä¢ {mitigation}")

    def _demo_crisis_resources(self):
        """Demonstrate crisis resource provision."""
        print("\nüÜò CRISIS RESOURCES DEMONSTRATION")
        print("-" * 30)

        resources = self.detector.get_crisis_resources()

        print("Crisis Intervention Resources:")
        for region, info in resources.items():
            print(f"\n{region.upper()} RESOURCES:")
            for resource_type, details in info.items():
                if isinstance(details, dict):
                    print(f"  üìû {details.get('description', resource_type)}")
                    if 'number' in details:
                        print(f"     Phone: {details['number']}")
                    if 'website' in details:
                        print(f"     Website: {details['website']}")
                else:
                    print(f"  ‚Ä¢ {resource_type}: {details}")

        print(f"\nCrisis Response Protocol:")
        crisis_steps = [
            "1. üö® Immediate Assessment: Determine severity and urgency",
            "2. üìû Resource Provision: Provide appropriate crisis contacts",
            "3. üë• Human Escalation: Alert oversight team for high-risk cases",
            "4. üìã Documentation: Log incident for monitoring and follow-up",
            "5. üîÑ Follow-up: Ensure appropriate care continuation"
        ]

        for step in crisis_steps:
            print(f"  {step}")

    def run_interactive_demo(self):
        """Run interactive demo allowing user input."""
        print("\nüéÆ INTERACTIVE DEMONSTRATION")
        print("=" * 50)

        print("Enter text to analyze (or 'quit' to exit):")
        print("Note: This is a simulation for educational purposes")

        while True:
            try:
                user_input = input("\nEnter text: ").strip()

                if user_input.lower() in ['quit', 'exit', 'q']:
                    break

                if not user_input:
                    print("Please enter some text to analyze.")
                    continue

                print(f"\nAnalyzing: \"{user_input}\"")

                # Use demo prediction mode
                prediction, probs = self.detector.predict_risk_demo(user_input)
                confidence = probs[1] if prediction == 1 else probs[0]

                print(f"Prediction: {'Risk Detected' if prediction == 1 else 'No Risk'}")
                print(f"Confidence: {confidence:.1%}")

                # Execute safety protocol
                risk_level = self.safety_guidelines.assess_risk_level(prediction, confidence)
                safety_results = self.safety_guidelines.execute_safety_protocol(
                    risk_level, user_input, {
                        'prediction': prediction,
                        'confidence': confidence,
                        'timestamp': 'interactive_demo'
                    }
                )

                print(f"Risk Level: {risk_level.value.upper()}")

                if safety_results['warnings_issued']:
                    print("\n‚ö†Ô∏è Safety Warnings:")
                    for warning in safety_results['warnings_issued']:
                        print(f"  ‚Ä¢ {warning}")

                if safety_results['resources_provided']:
                    print(f"\nüÜò Recommended Resources:")
                    for resource in safety_results['resources_provided'][:3]:
                        print(f"  ‚Ä¢ {resource}")

            except KeyboardInterrupt:
                print("\n\nDemo interrupted by user.")
                break
            except Exception as e:
                print(f"Error during analysis: {e}")

        print("\n‚úÖ Interactive demo completed.")

    def generate_demo_report(self):
        """Generate comprehensive demo report."""
        print("\nüìÑ DEMO REPORT GENERATION")
        print("=" * 50)

        report = f"""
        MENTAL HEALTH NLP SAFETY DEMO - COMPREHENSIVE REPORT
        ==================================================

        DEMO OVERVIEW:
        This demonstration showcased a responsible AI system for mental health
        applications, specifically suicide risk detection using LSTM neural networks.

        KEY TECHNICAL DEMONSTRATIONS:
        ‚Ä¢ Model Architecture: Bidirectional LSTM for text classification
        ‚Ä¢ Safety Integration: Comprehensive protocols for all risk levels
        ‚Ä¢ Ethical Framework: Multi-layered responsible AI implementation
        ‚Ä¢ Target Performance: >90% F1 score (based on literature benchmarks)

        SAFETY FEATURES DEMONSTRATED:
        ‚Ä¢ Real-time risk assessment and protocol execution
        ‚Ä¢ Crisis intervention resource provision
        ‚Ä¢ Human oversight requirements for high-risk cases
        ‚Ä¢ Comprehensive incident logging and monitoring

        ETHICAL CONSIDERATIONS:
        ‚Ä¢ Privacy protection through data minimization
        ‚Ä¢ Bias mitigation and fairness assessment
        ‚Ä¢ Transparency in limitations and uncertainty
        ‚Ä¢ Professional oversight and validation requirements

        RESPONSIBLE AI PRACTICES:
        ‚Ä¢ Clear usage guidelines and prohibited applications
        ‚Ä¢ Comprehensive safety monitoring and audit trails
        ‚Ä¢ Crisis intervention protocols and resource provision
        ‚Ä¢ Continuous performance and bias assessment

        TECHNICAL COMPETENCIES DEMONSTRATED:
        ‚Ä¢ Deep learning model development and optimization
        ‚Ä¢ Production-ready code with comprehensive error handling
        ‚Ä¢ Safety-critical system design and implementation
        ‚Ä¢ Ethical AI development and responsible deployment

        UNIQUE VALUE PROPOSITION:
        This project demonstrates the ability to:
        1. Develop high-performance ML models for sensitive applications
        2. Implement comprehensive safety and ethical frameworks
        3. Bridge technical excellence with social responsibility
        4. Design systems suitable for high-stakes domains

        CONCLUSION:
        This demonstration represents a unique combination of technical expertise
        and ethical responsibility, showcasing both ML engineering capabilities
        and deep understanding of responsible AI development practices.

        The project illustrates how clinical psychology insights can inform
        AI safety development, creating systems that are both technically
        excellent and socially responsible.
        """

        print(report)

        # Save report to file
        report_path = Path("demo_report.txt")
        with open(report_path, 'w') as f:
            f.write(report)

        print(f"\nüìÅ Full report saved to: {report_path}")


def main():
    """Main function with command-line interface."""
    parser = argparse.ArgumentParser(description="Mental Health NLP Safety Demo")

    parser.add_argument("--mode", choices=["demo", "interactive", "report"],
                       default="demo", help="Demo mode to run")
    parser.add_argument("--show-warnings", action="store_true",
                       help="Show all ethical warnings")

    args = parser.parse_args()

    # Suppress warnings if not requested
    if not args.show_warnings:
        warnings.filterwarnings("ignore")

    # Initialize demo
    demo = MentalHealthNLPDemo()

    # Run requested mode
    if args.mode == "demo":
        demo.run_demo_mode()
    elif args.mode == "interactive":
        demo.run_interactive_demo()
    elif args.mode == "report":
        demo.generate_demo_report()

    print(f"\nüéì Mental Health NLP Demo completed in '{args.mode}' mode.")
    print("This project demonstrates responsible AI development for sensitive domains.")


if __name__ == "__main__":
    main()